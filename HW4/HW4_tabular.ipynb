{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4_tabular.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y3z8kT9WqyPy",
        "outputId": "3f0738c9-f93c-4993-8bcc-5230e49a8b00"
      },
      "source": [
        "!pip install deep_autoviml"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep_autoviml\n",
            "  Downloading deep_autoviml-0.0.68-py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 15.4 MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.23.1\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 67.7 MB/s \n",
            "\u001b[?25hCollecting storm-tuner>=0.0.8\n",
            "  Downloading storm_tuner-0.0.9-py3-none-any.whl (14 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 21.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from deep_autoviml) (2019.12.20)\n",
            "Collecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 34 kB/s \n",
            "\u001b[?25hCollecting tensorflow-text==2.5.0\n",
            "  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 41.7 MB/s \n",
            "\u001b[?25hCollecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub==0.12.0 in /usr/local/lib/python3.7/dist-packages (from deep_autoviml) (0.12.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from deep_autoviml) (5.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from deep_autoviml) (3.2.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from deep_autoviml) (1.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from deep_autoviml) (1.1.5)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (from deep_autoviml) (1.1.0)\n",
            "Collecting tensorflow==2.5.1\n",
            "  Downloading tensorflow-2.5.1-cp37-cp37m-manylinux2010_x86_64.whl (454.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.4 MB 8.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (3.17.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (1.15.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (0.12.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (0.2.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (2.6.0)\n",
            "Collecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 44.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (1.12.1)\n",
            "Collecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (0.37.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.1->deep_autoviml) (1.12)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.1->deep_autoviml) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.1->deep_autoviml) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.1->deep_autoviml) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.1->deep_autoviml) (3.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->deep_autoviml) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->deep_autoviml) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->deep_autoviml) (5.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->deep_autoviml) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->deep_autoviml) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->deep_autoviml) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->deep_autoviml) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deep_autoviml) (0.2.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->deep_autoviml) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->deep_autoviml) (7.6.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->deep_autoviml) (4.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->deep_autoviml) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->deep_autoviml) (5.1.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->deep_autoviml) (5.6.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->deep_autoviml) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->deep_autoviml) (5.3.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->deep_autoviml) (5.1.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->deep_autoviml) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->deep_autoviml) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->deep_autoviml) (1.0.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->deep_autoviml) (4.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->deep_autoviml) (2.6.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->deep_autoviml) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->deep_autoviml) (0.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->deep_autoviml) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->deep_autoviml) (22.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->deep_autoviml) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->deep_autoviml) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->deep_autoviml) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deep_autoviml) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deep_autoviml) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deep_autoviml) (0.10.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep_autoviml) (4.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep_autoviml) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep_autoviml) (0.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep_autoviml) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep_autoviml) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep_autoviml) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->deep_autoviml) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->deep_autoviml) (21.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.5.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.4-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna->deep_autoviml) (4.62.3)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.9.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna->deep_autoviml) (3.13)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->deep_autoviml) (1.4.25)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->deep_autoviml) (1.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->deep_autoviml) (5.2.2)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->deep_autoviml) (2.2.1)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.4.0-py3-none-any.whl (20 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 74.1 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.2.0-py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 59.1 MB/s \n",
            "\u001b[?25hCollecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->deep_autoviml) (21.2.0)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->deep_autoviml) (2018.9)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->deep_autoviml) (1.11.2)\n",
            "Building wheels for collected packages: emoji, pyperclip\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=4c2014e14e8ec202d9f45db1d83883a407bf66b258f8feed9793a41dbd12dbce\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=1c96ec92753b6e808e265c38b8de8f90fd2504cc0b7712019ac92baea7492f4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built emoji pyperclip\n",
            "Installing collected packages: pyperclip, pbr, numpy, grpcio, colorama, tensorflow-estimator, stevedore, Mako, keras-nightly, cmd2, autopage, threadpoolctl, tensorflow, colorlog, cmaes, cliff, alembic, tensorflow-text, storm-tuner, scikit-learn, optuna, emoji, deep-autoviml\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.41.0\n",
            "    Uninstalling grpcio-1.41.0:\n",
            "      Successfully uninstalled grpcio-1.41.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Mako-1.1.5 alembic-1.7.4 autopage-0.4.0 cliff-3.9.0 cmaes-0.8.2 cmd2-2.2.0 colorama-0.4.4 colorlog-6.5.0 deep-autoviml-0.0.68 emoji-1.6.1 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 numpy-1.19.2 optuna-2.10.0 pbr-5.6.0 pyperclip-1.8.2 scikit-learn-1.0.1 stevedore-3.5.0 storm-tuner-0.0.9 tensorflow-2.5.1 tensorflow-estimator-2.5.0 tensorflow-text-2.5.0 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym7p3NexuqQK",
        "outputId": "ede8d7d2-23e0-4fa0-a5ac-8f0415bb9512"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "LLB = LabelEncoder()\n",
        "from deep_autoviml import deep_autoviml as deepauto"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imported deep_auto_viml. version=0.0.68\n",
            "from deep_autoviml import deep_autoviml as deepauto\n",
            "-------------------\n",
            "model, cat_vocab_dict = deepauto.fit(train, target, keras_model_type=\"fast\",\n",
            "\t\tproject_name=\"deep_autoviml\", keras_options=keras_options,  \n",
            "\t\tmodel_options=model_options, save_model_flag=True, use_my_model='',\n",
            "\t\tmodel_use_case='', verbose=0)\n",
            "\n",
            "predictions = deepauto.predict(model, project_name, test_dataset=test,\n",
            "                                 keras_model_type=keras_model_type, \n",
            "                                 cat_vocab_dict=cat_vocab_dict)\n",
            "                                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyZ-yFJ6us2T"
      },
      "source": [
        "datapath = '/content/'\n",
        "filename = 'train.csv'\n",
        "sep = ','\n",
        "train_datafile = datapath+filename\n",
        "target = 'Survived'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "u3dhs36PuzLX",
        "outputId": "701f033e-51a8-48cc-bd65-b7cd2c3c1f85"
      },
      "source": [
        "train = pd.read_csv(train_datafile, header=0, sep=sep)\n",
        "print(train.shape)\n",
        "train.head(1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(891, 12)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass                     Name   Sex   Age  SibSp  \\\n",
              "0            1         0       3  Braund, Mr. Owen Harris  male  22.0      1   \n",
              "\n",
              "   Parch     Ticket  Fare Cabin Embarked  \n",
              "0      0  A/5 21171  7.25   NaN        S  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "EX01BPSKvbXc",
        "outputId": "643d97e2-7760-412c-ac22-aded32ca4896"
      },
      "source": [
        "filename = 'test.csv'\n",
        "url = datapath + filename\n",
        "test = pd.read_csv(url, header=0, sep=sep)\n",
        "#train, test = split_data_n_ways(train, target, n_splits=2)\n",
        "print(train.shape, test.shape)\n",
        "test.head(1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(891, 12) (418, 11)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass              Name   Sex   Age  SibSp  Parch  Ticket  \\\n",
              "0          892       3  Kelly, Mr. James  male  34.5      0      0  330911   \n",
              "\n",
              "     Fare Cabin Embarked  \n",
              "0  7.8292   NaN        Q  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnnHunNbveEw"
      },
      "source": [
        "keras_model_type = \"auto\" ## always try \"fast\" first, then \"fast1\", \"fast2\", \"auto\" in that order\n",
        "project_name = \"taxi\"\n",
        "model_options = {'nlp_char_limit':50, 'cat_feat_cross_flag':\"\",\n",
        "                 'max_trials': 5, \"tuner\": \"storm\"}\n",
        "keras_options = {\"patience\":10, 'class_weight': True, 'early_stopping': True, \n",
        "                 'lr_scheduler': '', \"optimizer\": 'RMS'}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KA2exgLWvft8",
        "outputId": "dd1274dc-120d-4896-dd1c-20c8b864b828"
      },
      "source": [
        "model, cat_vocab_dict = deepauto.fit(train_datafile, target, keras_model_type=keras_model_type,\n",
        "\t\tproject_name=project_name, keras_options=keras_options, model_options=model_options, \n",
        "\t\tsave_model_flag=False, use_my_model='', verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Only CPU found on this device\n",
            "Setting CPU strategy using 1 devices\n",
            "Using following keras_options given as input:\n",
            "    patience : 10\n",
            "    optimizer : RMS\n",
            "    class_weight : True\n",
            "    lr_scheduler : \n",
            "    early_stopping : True\n",
            "Using following model_options given as input:\n",
            "    cat_feat_cross_flag : \n",
            "    nlp_char_limit : 50\n",
            "    max_trials : 5\n",
            "    tuner : storm\n",
            "Max Trials : 5. Please increase max_trials if you want to better accuracy...\n",
            "\n",
            "#################################################################################\n",
            "###########     L O A D I N G    D A T A    I N T O   TF.DATA.DATASET H E R E  #\n",
            "#################################################################################\n",
            "        \n",
            "CSV file being loaded into tf.data.Dataset\n",
            "    Since number of rows in file <= 10000 maxrows, loading entire file into pandas for EDA\n",
            "Alert! Modified column names to satisfy rules for column names in Tensorflow...\n",
            "    Model type is Classification and Single_Label problem\n",
            "Loaded a small data sample of size = (891, 12) into pandas dataframe to analyze...\n",
            "    Classifying variables using data sample in pandas...\n",
            "Alert! cabin has 2 mixed data types: Index([<class 'float'>, <class 'str'>], dtype='object') \n",
            "Alert! embarked has 2 mixed data types: Index([<class 'str'>, <class 'float'>], dtype='object') \n",
            "############## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "Classifying variables in data set...\n",
            "    Number of Numeric Columns =  2\n",
            "    Number of Integer-Categorical Columns =  3\n",
            "    Number of String-Categorical Columns =  1\n",
            "    Number of Factor-Categorical Columns =  0\n",
            "    Number of String-Boolean Columns =  1\n",
            "    Number of Numeric-Boolean Columns =  0\n",
            "    Number of Discrete String Columns =  2\n",
            "    Number of NLP String Columns =  0\n",
            "    Number of Date Time Columns =  0\n",
            "    Number of ID Columns =  2\n",
            "    Number of Columns to Delete =  2\n",
            "    11 Predictors classified...\n",
            "    possible latitude columns in dataset: []\n",
            "        after further analysis, no latitude columns found\n",
            "    possible longitude columns in dataset: []\n",
            "        after further analysis, no longitude columns found\n",
            "        2 variable(s) to be removed since they were ID or low-information variables\n",
            "    List of variables to be removed: ['passengerid', 'name']\n",
            "Distribution of string columns in datatset:\n",
            "    number of binary = 1, cats = 1, high cats = 0, very high cats = 3\n",
            "Distribution of integer columns in datatset:\n",
            "    number of binary = 1, cats = 3, high cats = 0, very high cats = 1\n",
            "Distribution of floats:\n",
            "    number float variables = 2\n",
            "Data Transformation Advisory:\n",
            "    performing integer feature crosses: changed cat_feat_cross_flag to \"num\" \n",
            "    we transformed ['passengerid'] from integer to float\n",
            "printing first five values of survived: [0 1 1 1 0]\n",
            "    after data cleaning, number of predictors used in modeling = 9\n",
            "Not performing feature crossing for categorical nor integer variables\n",
            "    866 missing values in dataset: filling them with default values...\n",
            "Loading your input file(s) data directly into tf.data.Dataset...\n",
            "    train data loaded successfully.\n",
            "\n",
            "Number of predictors to be used = 9 in predict step: keras preprocessing...\n",
            "Dropping ['passengerid', 'name'] columns marked for deletion...\n",
            "Boolean column successfully processed\n",
            "No NLP vars in data set. No preprocessing done.\n",
            "Printing one batch from the dataset:\n",
            "passengerid                             : [1 2 3 4]\n",
            "pclass                                  : [3 1 3 1]\n",
            "name                                    : [b'Braund, Mr. Owen Harris'\n",
            " b'Cumings, Mrs. John Bradley (Florence Briggs Thayer)'\n",
            " b'Heikkinen, Miss. Laina' b'Futrelle, Mrs. Jacques Heath (Lily May Peel)']\n",
            "sex                                     : [b'male' b'female' b'female' b'female']\n",
            "age                                     : [22. 38. 26. 35.]\n",
            "sibsp                                   : [1 1 0 1]\n",
            "parch                                   : [0 0 0 0]\n",
            "ticket                                  : [b'A/5 21171' b'PC 17599' b'STON/O2. 3101282' b'113803']\n",
            "fare                                    : [ 7.25  71.283  7.925 53.1  ]\n",
            "cabin                                   : [b'missing' b'C85' b'missing' b'C123']\n",
            "embarked                                : [b'S' b'C' b'S' b'S']\n",
            "       Class  -> Counts -> Percent\n",
            "           0:     549  ->   61.6%\n",
            "           1:     342  ->   38.4%\n",
            "    Class weights calculated: {0: 1.0, 1: 1.3026315789473684}\n",
            "\n",
            "#################################################################################\n",
            "###########     K E R A S     F E A T U R E    P R E P R O C E S S I N G  #######\n",
            "#################################################################################\n",
            "        \n",
            "There are no NLP variables in this dataset for preprocessing...\n",
            "Preprocessing non-NLP layers for auto Keras model...\n",
            "    auto combined wide and deep  successfully...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-588000b2b802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model, cat_vocab_dict = deepauto.fit(train_datafile, target, keras_model_type=keras_model_type,\n\u001b[1;32m      2\u001b[0m                 \u001b[0mproject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \t\tsave_model_flag=False, use_my_model='', verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deep_autoviml/deep_autoviml.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train_data_or_file, target, keras_model_type, project_name, save_model_flag, model_options, keras_options, use_my_model, model_use_case, verbose)\u001b[0m\n\u001b[1;32m    398\u001b[0m                                                 \u001b[0mcat_vocab_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras_model_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                                                 \u001b[0mkeras_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                                                 verbose)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_use_case\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deep_autoviml/preprocessing/preprocessing.py\u001b[0m in \u001b[0;36mperform_preprocessing\u001b[0;34m(train_ds, var_df, cat_vocab_dict, keras_model_type, keras_options, model_options, verbose)\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;31m#                                    kernel_regularizer=regularizers.l2(0.01)))(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    %s combined wide and deep  successfully...'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mkeras_model_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0mnlp_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnlp_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
          ]
        }
      ]
    }
  ]
}